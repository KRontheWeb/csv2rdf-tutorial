{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV to RDF Conversion Example\n",
    "\n",
    "CSV is by far the most common format of \"open data\" on the web. This small tutorial shows how CSV can be converted to RDF in a programmatic fashion. Other tools exist, such as [OpenRefine](http://openrefine.org) with the [RDF extension](https://github.com/fadmaa/grefine-rdf-extension/releases) or [LODRefine](https://github.com/sparkica/LODRefine) (which has RDF support built-in) but they do not really scale beyond simple datasets, and do not offer a repeatable framework (as e.g. can be used in a conversion pipeline).\n",
    "\n",
    "### Reading a CSV file\n",
    "\n",
    "Make sure the file `example.csv` is in the same directory as this IPython notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "filename = \"example.csv\"\n",
    "\n",
    "with open(filename,'r') as csvfile:\n",
    "    # Set the right quote character and delimiter\n",
    "    csvreader = csv.reader(csvfile,quotechar='\"',delimiter=';')\n",
    "    \n",
    "    # If the first row contains header information, we can retrieve it like so:\n",
    "    header = csvreader.next()\n",
    "    print \"Header\"\n",
    "    print header\n",
    "    \n",
    "    print \"Lines\"\n",
    "    for line in csvreader:\n",
    "        # Line is an array of the columns in the file\n",
    "        # Make sure to check the encoding of the strings in the array... this often causes issues\n",
    "        print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use a `csv.DictReader` object to turn the entire CSV file into a list of dictionaries. Note that this will load the CSV file into memory. For large CSV files, it is better to process the file line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from csv import DictReader\n",
    "\n",
    "filename = \"example.csv\"\n",
    "\n",
    "with open(filename,'r') as csvfile:\n",
    "    csv_contents = [{k: v for k, v in row.items()}\n",
    "        for row in csv.DictReader(csvfile, skipinitialspace=True, quotechar='\"', delimiter=';')]\n",
    "    \n",
    "print csv_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up stuff for RDF\n",
    "\n",
    "We import the things we'll need from `rdflib`:\n",
    "\n",
    "* `Dataset` is the object in which we will store our RDF graphs\n",
    "* `URIRef` is the datatype for URI-resources\n",
    "* `Literal` is the datatype for literal resources (strings, dates etc.)\n",
    "* `Namespace` is used to create namespaces (parts of the URI's we are going to make)\n",
    "* `RDF`, `RDFS`, `OWL` and `XSD` are built in namespaces\n",
    "\n",
    "**NB**: We'll use \"group 20\" for this example, but you should replace it with your own group name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rdflib import Dataset, URIRef, Literal, Namespace, RDF, RDFS, OWL, XSD\n",
    "\n",
    "# A namespace for our resources\n",
    "data = 'http://data.krw.d2s.labs.vu.nl/group20/resource/'\n",
    "DATA = Namespace(data)\n",
    "# A namespace for our vocabulary items (schema information, RDFS, OWL classes and properties etc.)\n",
    "vocab = 'http://data.krw.d2s.labs.vu.nl/group20/vocab/'\n",
    "VOCAB = Namespace('http://data.krw.d2s.labs.vu.nl/group20/vocab/')\n",
    "\n",
    "# The URI for our graph\n",
    "graph_uri = URIRef('http://data.krw.d2s.labs.vu.nl/group20/resource/examplegraph')\n",
    "\n",
    "# We initialize a dataset, and bind our namespaces\n",
    "dataset = Dataset()\n",
    "dataset.bind('g20data',DATA)\n",
    "dataset.bind('g20vocab',VOCAB)\n",
    "\n",
    "# We then get a new graph object with our URI from the dataset.\n",
    "graph = dataset.graph(graph_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make some RDF from our CSV Dictionary\n",
    "\n",
    "A straightforward conversion:\n",
    "\n",
    "* Make sure you have URIRef objects for all resources you want to make\n",
    "* Make sure you have Literal objects for all literal values you need. Be sure to use the proper datatype or a language tag.\n",
    "* Decide on what URI will be the 'primary key' for each row.\n",
    "* Decide on the terms you are going to use to create the relations (predicates, properties)\n",
    "* Add the triples to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IRI baker is a library that reliably creates valid (parts of) IRIs from strings (spaces are turned into underscores, etc.).\n",
    "from iribaker import to_iri\n",
    "\n",
    "# Let's iterate over the dictionary, and create some triples\n",
    "# Let's pretend we know exactly what the 'schema' of our CSV file is\n",
    "for row in csv_contents:\n",
    "    # `Name` is the primary key and we use it as our primary resource, but we'd also like to use it as a label\n",
    "    person = URIRef(to_iri(data + row['Name']))\n",
    "    name = Literal(row['Name'], datatype=XSD['string'])\n",
    "    # `Country` is a resource\n",
    "    country = URIRef(to_iri(data + row['Country']))\n",
    "    # But we'd also like to use the name as a label (with a language tag!)\n",
    "    country_name = Literal(row['Country'], lang='en')\n",
    "    # `Age` is a literal (an integer)\n",
    "    age = Literal(int(row['Age']), datatype=XSD['int'])\n",
    "    # `Favourite Colour` is a resource\n",
    "    colour = URIRef(to_iri(data + row['Favourite Colour']))\n",
    "    colour_name = Literal(row['Favourite Colour'], lang='en')\n",
    "    # `Place` is a resource\n",
    "    place = URIRef(to_iri(data+ row['Place']))\n",
    "    place_name = Literal(row['Place'], lang='en')\n",
    "    # `Address` is a literal (a string)\n",
    "    address = Literal(row['Address'], datatype=XSD['string'])\n",
    "    # `Hobby` is a resource\n",
    "    hobby = URIRef(to_iri(data + row['Hobby']))\n",
    "    hobby_name = Literal(row['Hobby'], lang='en')\n",
    "    \n",
    "    # All set... we are now going to add the triples to our graph\n",
    "    graph.add((person, VOCAB['name'], name))\n",
    "    graph.add((person, VOCAB['age'], age))\n",
    "    graph.add((person, VOCAB['address'], address))\n",
    "    \n",
    "    # Add the place and its label\n",
    "    graph.add((person, VOCAB['place'], place))\n",
    "    graph.add((place, VOCAB['name'], place_name))\n",
    "    \n",
    "    # Add the country and its label\n",
    "    graph.add((person, VOCAB['country'], country))\n",
    "    graph.add((country, VOCAB['name'], country_name))\n",
    "    \n",
    "    # Add the favourite colour and its label\n",
    "    graph.add((person, VOCAB['favourite_colour'], colour))\n",
    "    graph.add((colour, VOCAB['name'], colour_name))\n",
    "    \n",
    "    # Add the hobby and its label\n",
    "    graph.add((person, VOCAB['hobby'], hobby))\n",
    "    graph.add((hobby, VOCAB['name'], hobby_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this turned out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dataset.serialize(format='trig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the RDF to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('example-simple.trig','w') as f:\n",
    "    graph.serialize(f, format='trig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, with a bit more thought\n",
    "\n",
    "Actually, we were a bit naive just now.\n",
    "\n",
    "* We are implicitly defining a schema: all property names are schema information. We might want to include an (externally defined) schema information.\n",
    "* Some of these properties may have useful standard names (e.g. for the `g20vocab:name` property we can use `rdfs:label`).\n",
    "* We have not specified types for our URIs.\n",
    "* The CSV-file specific named graph may not be the best place for some of our information (e.g. the names of things that may occur in multiple graphs).\n",
    "* And we have 2 *different* Amsterdam resources... one in NL, the other one in the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clear the graph from the dataset (because we're going to start anew)\n",
    "dataset.remove_graph(graph)\n",
    "# And get a new object (with the same URI, to create some confusion)\n",
    "graph = dataset.graph(graph_uri)\n",
    "\n",
    "\n",
    "# Load the externally defined schema into the default graph (context) of the dataset\n",
    "dataset.default_context.parse('vocab.ttl', format='turtle')\n",
    "\n",
    "\n",
    "# Let's iterate over the dictionary, and create some triples\n",
    "# Let's pretend we know exactly what the 'schema' of our CSV file is\n",
    "for row in csv_contents:\n",
    "    # `Name` is the primary key and we use it as our primary resource, but we'd also like to use it as a label\n",
    "    person = URIRef(to_iri(data + row['Name']))\n",
    "    name = Literal(row['Name'], datatype=XSD['string'])\n",
    "    # `Country` is a resource\n",
    "    country = URIRef(to_iri(data + row['Country']))\n",
    "    # But we'd also like to use the name as a label (with a language tag!)\n",
    "    country_name = Literal(row['Country'], lang='en')\n",
    "    # `Age` is a literal (an integer)\n",
    "    age = Literal(int(row['Age']), datatype=XSD['int'])\n",
    "    # `Favourite Colour` is a resource\n",
    "    colour = URIRef(to_iri(data + row['Favourite Colour']))\n",
    "    colour_name = Literal(row['Favourite Colour'], lang='en')\n",
    "    # `Place` is a resource, but we are now going to prepend the country to avoid ambiguity\n",
    "    place = URIRef(to_iri(data + row['Country'] + '/' + row['Place']))\n",
    "    place_name = Literal(row['Place'], lang='en')\n",
    "    # `Address` is a literal (a string)\n",
    "    address = Literal(row['Address'], datatype=XSD['string'])\n",
    "    # `Hobby` is a resource\n",
    "    hobby = URIRef(to_iri(data + row['Hobby']))\n",
    "    hobby_name = Literal(row['Hobby'], lang='en')\n",
    "    \n",
    "    # All set... we are now going to add the triples to our graph\n",
    "    graph.add((person, RDFS.label, name))\n",
    "    graph.add((person, VOCAB['age'], age))\n",
    "    graph.add((person, VOCAB['address'], address))\n",
    "    \n",
    "    # Add the place, its label and its type.\n",
    "    graph.add((person, VOCAB['place'], place))\n",
    "    dataset.add((place, RDFS.label, place_name))\n",
    "    dataset.add((place, RDF.type, VOCAB['Place']))\n",
    "    \n",
    "    # Add the country and its label\n",
    "    graph.add((person, VOCAB['country'], country))\n",
    "    dataset.add((country, RDFS.label, country_name))\n",
    "    dataset.add((country, RDF.type, VOCAB['Country']))\n",
    "    \n",
    "    # Add the favourite colour and its label\n",
    "    graph.add((person, VOCAB['favourite_colour'], colour))\n",
    "    dataset.add((colour, RDFS.label, colour_name))\n",
    "    dataset.add((colour, RDF.type, VOCAB['Colour']))\n",
    "    \n",
    "    # Add the hobby and its label\n",
    "    graph.add((person, VOCAB['hobby'], hobby))\n",
    "    dataset.add((hobby, RDFS.label, hobby_name))\n",
    "    dataset.add((hobby, RDF.type, VOCAB['Hobby']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dataset.serialize(format='trig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and Save the RDF to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('example-extended.trig','w') as f:\n",
    "    dataset.serialize(f, format='trig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But for the smartypants...\n",
    "\n",
    "We can actually do without a lot of the explicit assertion of types, since the domain and range definitions already provide us with the miminal information needed to infer the types of all of these things.\n",
    "\n",
    "Let's run the conversion again, without adding the types, and inspect the outcome in TopBraid (or Stardog).\n",
    "\n",
    "Since we've been adding stuff to the default graph, we have to re-initialize our dataset entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We initialize a fresh dataset, and bind our namespaces\n",
    "dataset = Dataset()\n",
    "dataset.bind('g20data',DATA)\n",
    "dataset.bind('g20vocab',VOCAB)\n",
    "\n",
    "# And get a new object (with the same URI, to create some confusion)\n",
    "graph = dataset.graph(graph_uri)\n",
    "\n",
    "\n",
    "# Load the externally defined schema into the default graph (context) of the dataset\n",
    "dataset.default_context.parse('vocab.ttl', format='turtle')\n",
    "\n",
    "\n",
    "# Let's iterate over the dictionary, and create some triples\n",
    "# Let's pretend we know exactly what the 'schema' of our CSV file is\n",
    "for row in csv_contents:\n",
    "    # `Name` is the primary key and we use it as our primary resource, but we'd also like to use it as a label\n",
    "    person = URIRef(to_iri(data + row['Name']))\n",
    "    name = Literal(row['Name'], datatype=XSD['string'])\n",
    "    # `Country` is a resource\n",
    "    country = URIRef(to_iri(data + row['Country']))\n",
    "    # But we'd also like to use the name as a label (with a language tag!)\n",
    "    country_name = Literal(row['Country'], lang='en')\n",
    "    # `Age` is a literal (an integer)\n",
    "    age = Literal(int(row['Age']), datatype=XSD['int'])\n",
    "    # `Favourite Colour` is a resource\n",
    "    colour = URIRef(to_iri(data + row['Favourite Colour']))\n",
    "    colour_name = Literal(row['Favourite Colour'], lang='en')\n",
    "    # `Place` is a resource, but we are now going to prepend the country to avoid ambiguity\n",
    "    place = URIRef(to_iri(data + row['Country'] + '/' + row['Place']))\n",
    "    place_name = Literal(row['Place'], lang='en')\n",
    "    # `Address` is a literal (a string)\n",
    "    address = Literal(row['Address'], datatype=XSD['string'])\n",
    "    # `Hobby` is a resource\n",
    "    hobby = URIRef(to_iri(data + row['Hobby']))\n",
    "    hobby_name = Literal(row['Hobby'], lang='en')\n",
    "    \n",
    "    # All set... we are now going to add the triples to our graph\n",
    "    graph.add((person, RDFS.label, name))\n",
    "    graph.add((person, VOCAB['age'], age))\n",
    "    graph.add((person, VOCAB['address'], address))\n",
    "    \n",
    "    # Add the place, its label and its type.\n",
    "    graph.add((person, VOCAB['place'], place))\n",
    "    dataset.add((place, RDFS.label, place_name))\n",
    "    \n",
    "    # Add the country and its label\n",
    "    graph.add((person, VOCAB['country'], country))\n",
    "    dataset.add((country, RDFS.label, country_name))\n",
    "    \n",
    "    # Add the favourite colour and its label\n",
    "    graph.add((person, VOCAB['favourite_colour'], colour))\n",
    "    dataset.add((colour, RDFS.label, colour_name))\n",
    "    \n",
    "    # Add the hobby and its label\n",
    "    graph.add((person, VOCAB['hobby'], hobby))\n",
    "    dataset.add((hobby, RDFS.label, hobby_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that indeed the types are missing from the named graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dataset.serialize(format='trig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And Save the RDF again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('example-inferencing.trig','w') as f:\n",
    "    dataset.serialize(f, format='trig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to Stardog\n",
    "\n",
    "We now upload the RDF to Stardog, to see how this inferencing thing works (we're using the same code as used in the [Web Application tutorial](https://github.com/KRontheWeb/web-application)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "TUTORIAL_REPOSITORY = \"http://stardog.krw.d2s.labs.vu.nl/group20\"\n",
    "\n",
    "def upload_to_stardog(data):\n",
    "    transaction_begin_url = TUTORIAL_REPOSITORY + \"/transaction/begin\"\n",
    "    \n",
    "    # Start the transaction, and get a transaction_id\n",
    "    response = requests.post(transaction_begin_url, headers={'Accept': 'text/plain'})\n",
    "    transaction_id = response.content\n",
    "\n",
    "    # POST the data to the transaction\n",
    "    post_url = TUTORIAL_REPOSITORY + \"/\" + transaction_id + \"/add\"\n",
    "    response = requests.post(post_url, data=data, headers={'Accept': 'text/plain', 'Content-type': 'application/trig'})\n",
    "\n",
    "    # Close the transaction\n",
    "    transaction_close_url = TUTORIAL_REPOSITORY + \"/transaction/commit/\" + transaction_id\n",
    "    response = requests.post(transaction_close_url)\n",
    "\n",
    "    return str(response.status_code)\n",
    "\n",
    "# Upload the serialization of our dataset to Stardog\n",
    "upload_to_stardog(dataset.serialize(format='trig'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see whether we can query Stardog for the 'inferred' information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "query = \"\"\"\n",
    "PREFIX gr20vocab: <http://data.krw.d2s.labs.vu.nl/group20/vocab/> \n",
    "\n",
    "SELECT * WHERE {\n",
    "  ?x a gr20vocab:Person .\n",
    "}\"\"\"\n",
    "\n",
    "endpoint = TUTORIAL_REPOSITORY + '/query'\n",
    "\n",
    "sparql = SPARQLWrapper(endpoint)\n",
    "\n",
    "sparql.setQuery(query)\n",
    "\n",
    "sparql.setReturnFormat(JSON)\n",
    "sparql.addParameter('Accept','application/sparql-results+json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without inferencing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparql.addParameter('reasoning','false')\n",
    "response = sparql.query().convert()\n",
    "print response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparql.addParameter('reasoning','true')\n",
    "response = sparql.query().convert()\n",
    "print response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
